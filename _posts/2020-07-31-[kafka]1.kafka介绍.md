---
layout: post
title:  "[kafka]1.kafka介绍"
date:   2020-07-31 17:02
tags: kafka
color: rgb(255,90,90)
cover: '../coverimages/kafka.jpg'
---

**介绍**
> Apache Kafka是 一个分布式流处理平台

**topic 和 日志**
> topic 就是数据主题 ，是数据记录发布的地方。kafka中的topic是多订阅者模式，一个topic可以被多个消费者订阅，对于每一个topic，kafka集群都会维护一个分区日志，如下图:


![enter description here](https://raw.githubusercontent.com/LazystudentCH/blogImage/master/2020/7/31/[kafka]1.kafka介绍/1596183463753.png)

> 每一个分区有序且不可变的结果记录集，并且不断追加的log文件。分区中的每一个记录都会分配一个ID号来标识顺序，我们把这个叫做offset，offset可以用来标识分区中的唯一一条记录

> kafka会保存所有的发布记录，无论这些消息是否被消费，同时可以通过一个参数来配置保留的期限。例如：保留期限设置为两天，一条记录发布之后的两天内，可以随时被消费，两天后这条记录会被抛弃并且会释放磁盘空间。kafka的性能和数据大小无关，所以可以长时间存储数据。

> 事实上，在每一个消费者中唯一保存的元数据是offset（偏移量）即消费在log中的位置，kafka的消费其实是根据offset来读取数据，读完一条数据offer往后移。所以，消费者可以采取任意的顺序来消费数据。例如，两天前消费过的数据，可以在现在将offset往回移动，从而重新消费过去的数据

![enter description here](https://raw.githubusercontent.com/LazystudentCH/blogImage/master/2020/7/31/[kafka]1.kafka介绍/1596184915735.png)

> 这些细节说明，kafka的消费者是非常廉价的，消费者数量的增加或者减少，对集群或者其他消费者是没有多大影响的。比如你可以使用一些命令来直接获取数据，例如tail之类的，并不会影响其他消费者来消费数据


> 日志中的分区，有如下几个用途，第一，当日志的数量超过了单台服务器的限制，允许日志进行扩展。每个单独的分区都必须受限于主机的文件限制，不过一个主题可能有多个分区，因此可以处理无限量的数据

**分布式**
> 日志的分区在kafka的集群的服务器上，每个服务器在处理数据和请求时，共享这些分区。每一个分区都会在已配置的服务器上进行备份，确保容错性。
> 每个分区都有一个leader，零个或者多个follwers。leader处理一切对于分区的读写请求，而follwer只需要被动的同步leader上面的数据.。当leader宕机的时候，follwer中的一台服务器会成为新的leader。

**生产者**
> 生产者可以将数据发布到所选择的topic中。生产者将负责记录分配到topic中的哪一个分区中，可以使用循环的方式来简单的实现负载均衡，也可以根据某些函数来完成。

**消费者**

> 消费者使用一个消费者组名称来进行标识，消费者组包含一个或者多个消费者实例，发布到topic中的每一条消息，将会被分配给订阅了这个topic的消费者组中的一个消费者实例，如果所有的消费者实例在同一消费组中，消息记录会负载平衡到每一个消费者实例，如果所有的消费者实例在不同的消费组中，每条消息记录会广播到所有的消费者进程.

![enter description here](https://raw.githubusercontent.com/LazystudentCH/blogImage/master/2020/8/2/[kafka]1.kafka介绍/1596378832325.png)

> 如上图，这个kafka集群有两台server，四个分区，和2个消费者组
> 通常情况下，每个topic都会有一些消费者组，一个消费者组对应一个“逻辑订阅者”。一个消费者组由许多消费者实例组成，便于扩展和容错。
>在kafka中实现消费的方式是将日志中的分区划分到每一个消费者实例上，以便在任何时间，每个消费组内都有一个唯一的消费者实例。维护消费者中的消费关系由kafka协议动态处理。如果新的实例加入组，他们将从族中其他成员处接管一些分区；如果一个实例消失，这个消失实例所拥有的分区将被发送到其他实例
kafka只保证分区内的记录是有序的，而不保证主题中不同的分区有序

