---
layout: post
title:  "[kafka]3.架构深入"
date:   2020-08-04 12:37
tags: kafka
color: rgb(255,90,90)
cover: '../coverimages/kafka.jpg'
---

# 架构深入

**kafka工作流程以及文件存储**
![enter description here](https://raw.githubusercontent.com/LazystudentCH/blogImage/master/2020/8/3/[kafka]3.架构深入/1596446484099.png)

> kafka中的消息都是以topic进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。
> topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是producer生产的数据。Producer生产的数据会被不断的追加到该log文件的末端，且每条数据都有自己的offset。消费者组的每个消费者，都会时实记录自己消费到了哪个offset，以便出错时，从上次的位置继续消费。

![enter description here](https://raw.githubusercontent.com/LazystudentCH/blogImage/master/2020/8/3/[kafka]3.架构深入/1596451806552.png)

> 由于生产者会不断追加日志到log文件末尾，为了防止log文件过大导致数据定位效率低下，kafka采用了索引和分片的机制，将每个partition分解为多个segment(部分)。每个segment对应两个文件，".inde"和".log"文件，这些文件位于一个文件夹下，该文件夹的命名规则为topic名称+分区号。例如，first这个topic有三个分区，则其对应的文件夹为first-0,first-1,first-2。一个log文件默认最大为1g

![enter description here](https://raw.githubusercontent.com/LazystudentCH/blogImage/master/2020/8/3/[kafka]3.架构深入/1596452354105.png)

**查找数据**
> log文件只存当前分片的某个segment数据，而index文件存当前分片的某个segment索引
> log文件和索引文件的命名方式都是以当前最小偏移量来命名的（上一个文件的最大偏移量+1）
> 查找数据过程如下：
> 1.根据之前消费的偏移量，用二分法，查找出目标index文件
> 2.因为index中存的都是数据的开始偏移量以及数据长度和一些其他的数据（每一条记录长度固定），所以，可以快速的用 Nx固定长度（N代表第几条数据） 的方式快速获取到开始偏移量和数据长度
> 3.拿到初始偏移量和数据长度，直接去对应的log文件中取出数据

![enter description here](https://raw.githubusercontent.com/LazystudentCH/blogImage/master/2020/8/3/[kafka]3.架构深入/1596466171977.png)

**分区的原因**
> 1.提高可扩展性
> 2.提高高并发，因为能以partition为单位读写了

**分区的原则**
![enter description here](https://raw.githubusercontent.com/LazystudentCH/blogImage/master/2020/8/6/[kafka]3.架构深入/1596725732020.png)

> 1.第一种情况，指定了partition，直接发到这个partition
> 2.没有指定partition，但是有key的情况下，将key做hash 然后余上partition数量，得到的值就是即将发送到的partition
> 3. 既没有partition，也没有key ， 生成一个随机数，余上partition的数量，然后轮询

**可靠性保证**
> 为保证producer生产数据的可靠性，topic每个分片收到producer的数据后，都需要返回一个ack，如果producer收到ack，就会发送下一轮数据，否则重新发送数据

![enter description here](https://raw.githubusercontent.com/LazystudentCH/blogImage/master/2020/8/8/[kafka]3.架构深入/1596855934146.png)

![enter description here](https://raw.githubusercontent.com/LazystudentCH/blogImage/master/2020/8/8/[kafka]3.架构深入/1596855955561.png)

> kafka选择了第二种，全部完成同步，才发送ack，原因如下：
> 1.为了容忍n个节点的故障，第一种需要2n+1的副本，而第二种只需要n个
> 2.虽然第二种方案网络延迟要求较低，但是网络对kafka的影响较小

**ISR机制**
> 采用第二种方案之后，设想以下情景：leader 收到数据，所有 follower 都开始同步数据，但有一个 follower，因为某种故障，迟迟不能与 leader 进行同步，那 leader 就要一直等下去，直到它完成同步，才能发送 ack。这个问题怎么解决呢？

> Leader维护了一个动态的集合，称作in-sync replica set(ISR)，意思为和leader保持同步的集合。当ISR集合中的follower同步完成数据后，就给leader发送ack，当ISR中的所有follower都发送完ack后，leader返回给客户端ack，如果follower长时间没有给leader同步数据，那么将会被踢出ISR，进入ISR的标准是以与leader同步的时间作为依据。leader发生故障后，会从ISR中选取新的leader。

**ack应答机制**
> 对于一些不太重要的数据，对数据的可靠性要求不是很高，能够容忍少量数据丢失，所以没必要等ISR中的follower同步成功。
> kafka提供了三种级别可供用户选择，用户根据可靠性和延迟进行权衡

>acks参数配置:
>0:acks = 0代表 producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接受到还没有写入磁盘就已经返回，当broker故障的时候会丢失数据
>1:ack = 1 代表 producer等待ack，当leader落盘之后就返回，而不需要等待所有的follower落盘，如果在follower同步之前leader发生故障，数据有可能会丢失
>-1: acks = -1 代表 producer等待ack， follower和leader全部落盘成功后才返回ack，但是如果在follower同步之后，broker发送ack之前，leader出现故障，会造成数据重复

![enter description here](https://raw.githubusercontent.com/LazystudentCH/blogImage/master/2020/8/8/[kafka]3.架构深入/1596860986239.png)

**数据一致性(故障处理细节)**
> 每个broker（包括leader和follower）都有两个标志位，HW和LEO
> LEO：指的是每个副本最大的 offset；
HW：指的是消费者能见到的最大的 offset，ISR 队列中最小的 LEO。

![enter description here](https://raw.githubusercontent.com/LazystudentCH/blogImage/master/2020/8/9/[kafka]3.架构深入/1596948413833.png)

>follower发生故障:
>follower发生故障之后会被踢出ISR，等到该follower恢复之后，follower会读取本地磁盘记录的上次的HW，并将log文件中高于HW的部分截取掉，从HW开始向leader同步，当该follower的LEO追上leader的HW的时候，便可以重新加入ISR了。

> leader发生故障:
> leader发生故障之后，会从ISR中选出一个新的leader，之后，为了保证多个副本之间的数据一致性，其余的 follower 会先将各自的 log 文件高于 HW 的部分截掉，然后从新的 leader同步数据。

**Exactly Once**

> 将服务器的acks设置为-1，可以保证producer到server之间不会丢失数据，即at least once ，至少一次。相对的，将acks设置为0，可以producer每条消息只会被发送一次，即at most once ，最多一次。

> al least once 可以保证数据不丢失，但是不能保证数据不重复，而相对的，at most once 可以保证数据不重复但是不能保证数据不丢失。
> 但是，对于一些非常重要的信息，下游消费数据要求数据既不重复也不丢失，即exactly once 。在 0.11 版本以前的 Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。

> 0.11版本的kafka，引入了一项重大的特性，幂等性。所谓的幂等性就是不论producer发送多少次重复的数据，server都只会持久化一条。

> 要启用幂等性，需要在producer的参数中设置中 enable.idompotence 设置为 true 即可。kafka的幂等性实现，其实就是就是将原来下游需要做的去重放在了上游。开启幂等性的producer在初始化的时候会有一个PID(producerID)，发往同一个partition的消息会带上一个sequence number(序列号)。而broker会对<PID,seqNumber,partition>做缓存，用这三个值组成一个唯一的建，当这个值相同的时候，broker只会持久化一条。

> 但是 PID 重启就会变化，同时不同的 Partition 也具有不同主键，所以幂等性无法保证跨分区跨会话的 Exactly Once。
